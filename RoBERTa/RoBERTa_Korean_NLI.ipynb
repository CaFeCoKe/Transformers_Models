{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBERTa_Korean_NLI.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPJIRFIGkxBxpkNACTzrnGF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"7b2518946a4845438fb8b1575662e07e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b67dcbf39fe4719afcddffe3b244a00","IPY_MODEL_e8f755feb1ac4158909bb9615139a825","IPY_MODEL_0b7075b6f4954b519359f99c70b58ea9"],"layout":"IPY_MODEL_f9321e894e024fcbb5321cbe4b0ec5cc"}},"2b67dcbf39fe4719afcddffe3b244a00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a69fc6d401904bedbbd40c7f291b2728","placeholder":"​","style":"IPY_MODEL_eadbd378b6f74a8d93b6d5bd3114aeaf","value":"100%"}},"e8f755feb1ac4158909bb9615139a825":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a85883df04b64bcf9b5877a173b0eb88","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb5bbfc4b87a47e0ab6e709e7522cd32","value":2}},"0b7075b6f4954b519359f99c70b58ea9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0b53132fc4e4cd19117ec4f0a56fcf4","placeholder":"​","style":"IPY_MODEL_75a55d4125454647b64119babb06742f","value":" 2/2 [00:00&lt;00:00, 50.78it/s]"}},"f9321e894e024fcbb5321cbe4b0ec5cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a69fc6d401904bedbbd40c7f291b2728":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eadbd378b6f74a8d93b6d5bd3114aeaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a85883df04b64bcf9b5877a173b0eb88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb5bbfc4b87a47e0ab6e709e7522cd32":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0b53132fc4e4cd19117ec4f0a56fcf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75a55d4125454647b64119babb06742f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# RoBERTa를 이용한 한국어 자연어추론(NLI)\n","- 사전학습 모델 : KLUE-RoBERTa (MODU, CC-100-Kor, NAMUWIKI, NEWSCRAWL, PETITION)\n","- 데이터 : KLUE-NLI (WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, NSMC and AIRBNB)"],"metadata":{"id":"v64URFLfLZWI"}},{"cell_type":"markdown","source":["# 사전 준비"],"metadata":{"id":"eohmX-wwWF5c"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"G2iIuApKH7Ww","executionInfo":{"status":"ok","timestamp":1657860499634,"user_tz":-540,"elapsed":6754,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"959bb002-ba78-4e3b-aac0-ce1723daf89a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}],"source":["!pip install transformers\n","!pip install datasets"]},{"cell_type":"markdown","source":["**KLUE-NLI 데이터 불러오기**"],"metadata":{"id":"AkqIRoJjLQUL"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","datasets = load_dataset(\"klue\", \"nli\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["7b2518946a4845438fb8b1575662e07e","2b67dcbf39fe4719afcddffe3b244a00","e8f755feb1ac4158909bb9615139a825","0b7075b6f4954b519359f99c70b58ea9","f9321e894e024fcbb5321cbe4b0ec5cc","a69fc6d401904bedbbd40c7f291b2728","eadbd378b6f74a8d93b6d5bd3114aeaf","a85883df04b64bcf9b5877a173b0eb88","eb5bbfc4b87a47e0ab6e709e7522cd32","d0b53132fc4e4cd19117ec4f0a56fcf4","75a55d4125454647b64119babb06742f"]},"id":"N7FOH0lIKGZ5","executionInfo":{"status":"ok","timestamp":1657860500645,"user_tz":-540,"elapsed":1015,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"2bc13c49-4561-4de6-ca82-2db9a1fa9e40"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Reusing dataset klue (/root/.cache/huggingface/datasets/klue/nli/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b2518946a4845438fb8b1575662e07e"}},"metadata":{}}]},{"cell_type":"code","source":["datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0XX3vUCK8-P","executionInfo":{"status":"ok","timestamp":1657860500646,"user_tz":-540,"elapsed":6,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"91017494-eb1f-45e1-b1e2-28cbe66b7919"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n","        num_rows: 24998\n","    })\n","    validation: Dataset({\n","        features: ['guid', 'source', 'premise', 'hypothesis', 'label'],\n","        num_rows: 3000\n","    })\n","})"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# label 0: entailment(함의) / 1: neutral(중립) / 2: contradiction(모순)\n","print(datasets[\"train\"][0])\n","print(datasets[\"validation\"][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1X0cHXGTLHcu","executionInfo":{"status":"ok","timestamp":1657860500646,"user_tz":-540,"elapsed":5,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"688f62df-f5f7-4f95-fbf7-8de62554d88b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'guid': 'klue-nli-v1_train_00000', 'source': 'NSMC', 'premise': '힛걸 진심 최고다 그 어떤 히어로보다 멋지다', 'hypothesis': '힛걸 진심 최고로 멋지다.', 'label': 0}\n","{'guid': 'klue-nli-v1_dev_00000', 'source': 'airbnb', 'premise': '흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.', 'hypothesis': '어떤 방에서도 흡연은 금지됩니다.', 'label': 2}\n"]}]},{"cell_type":"markdown","source":["**KLUE-RoBERTa 모델과 토크나이저 불러오기**"],"metadata":{"id":"auZU-cTRLTY3"}},{"cell_type":"code","source":["from transformers import AutoModel, AutoTokenizer\n","\n","roberta_model = AutoModel.from_pretrained(\"klue/roberta-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P81m6GhYIJmP","executionInfo":{"status":"ok","timestamp":1657860505392,"user_tz":-540,"elapsed":4749,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"f439e7dc-5c68-482c-b686-bd2b2b39216a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["roberta_model.config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGeKL2qbIM_L","executionInfo":{"status":"ok","timestamp":1657860505392,"user_tz":-540,"elapsed":11,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"63df1a73-f7eb-49c7-f8bd-1f0b78d2935f"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaConfig {\n","  \"_name_or_path\": \"klue/roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertTokenizer\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# 토크나이징, 데이터 구축\n","\n","**스페셜 토큰 확인**"],"metadata":{"id":"-L372VdNRGLn"}},{"cell_type":"code","source":["for i in range (10):\n","    print(\"index : \",i,\" =  tokens : \",tokenizer.decode(i))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kCsB6RRhRHfB","executionInfo":{"status":"ok","timestamp":1657860507596,"user_tz":-540,"elapsed":2213,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"46f20c2c-9305-4189-9efd-12d8716b73dc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["index :  0  =  tokens :  [CLS]\n","index :  1  =  tokens :  [PAD]\n","index :  2  =  tokens :  [SEP]\n","index :  3  =  tokens :  [UNK]\n","index :  4  =  tokens :  [MASK]\n","index :  5  =  tokens :  !\n","index :  6  =  tokens :  \"\n","index :  7  =  tokens :  #\n","index :  8  =  tokens :  $\n","index :  9  =  tokens :  %\n"]}]},{"cell_type":"markdown","source":["**[CLS] 전제 [SEP] 가설 [SEP] [PAD]...**"],"metadata":{"id":"ciQGwlQsW2ON"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset"],"metadata":{"id":"KbtqUsOCYKtJ","executionInfo":{"status":"ok","timestamp":1657860507597,"user_tz":-540,"elapsed":10,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class NLIDataset(Dataset):\n","    def __init__(self, data, max_len=64):  # 데이터셋의 전처리를 해주는 부분\n","        self._data = data\n","        self.max_len = max_len\n","        self.bos = tokenizer.bos_token      # [CLS]\n","        self.eos = tokenizer.eos_token      # [SEP]\n","        self.pad = tokenizer.pad_token      # [PAD]\n","        self.sep = tokenizer.sep_token      # [SEP]\n","        self.tokenizer = tokenizer\n","    \n","    def __len__(self):\n","        return len(self._data)\n","\n","    def __getitem__(self, idx):  # 로드한 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n","        index = self._data[idx]\n","\n","        p = index[\"premise\"]  # 전제\n","        p_toked = self.tokenizer.tokenize(self.bos + p + self.sep)      # [CLS] 전제 [SEP]\n","        p_len = len(p_toked)\n","\n","        h = index[\"hypothesis\"]  # 가설\n","        h_toked = self.tokenizer.tokenize(h + self.eos)      # 가설 [SEP]\n","        h_len = len(p_toked)\n","\n","        # 전제 + 가설 길이가 최대길이보다 클때\n","        if p_len + h_len > self.max_len:\n","            h_len = self.max_len - p_len        # 가설의 길이 = 최대길이 - 전제길이\n","\n","            if p_len <= 0:       # 전제의 길이가 너무 길어 전제만으로 최대 길이를 초과 한다면\n","                p_toked = p_toked[-(int(self.max_len / 2)) :]   # 전제길이를 최대길이의 반으로 \n","                p_len = len(p_toked)\n","                h_len = self.max_len - p_len              # 답변의 길이를 최대길이 - 전제길이\n","                \n","            h_toked = h_toked[:h_len]\n","            h_len = len(h_toked)\n","\n","        # 전제 + 가설 토큰을 index로 변환   \n","        token_ids = self.tokenizer.convert_tokens_to_ids(p_toked + h_toked)\n","\n","        # 최대 길이만큼 padding\n","        while len(token_ids) < self.max_len:\n","            token_ids += [self.tokenizer.pad_token_id]\n","\n","        # attention_mask(어텐션마스크) = 전제 + 가설 길이 1 + 나머지(패딩) 0\n","        attention_mask = [1]*(p_len + h_len) + [0]*(self.max_len - p_len - h_len)\n","\n","        # label = 0: entailment(함의) / 1: neutral(중립) / 2: contradiction(모순)\n","        label = index[\"label\"]\n","\n","        # 전제+가설 + 답변, 어텐션마스크, label\n","        return (token_ids, attention_mask, label)"],"metadata":{"id":"1Z0XEqTbXBIY","executionInfo":{"status":"ok","timestamp":1657860507597,"user_tz":-540,"elapsed":9,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**데이터셋 구축** <br>\n","구성 : (token_ids, attention_mask, token_type_ids, label)"],"metadata":{"id":"w_x4SOhyjgLJ"}},{"cell_type":"code","source":["# 훈련 데이터셋\n","train_dataset = NLIDataset(datasets[\"train\"])\n","\n","for n in range(5):\n","    print(\"train_dataset[\",n,\"]\")\n","    print(\"token_ids      : \", train_dataset[n][0])\n","    print(\"attention_mask : \", train_dataset[n][1])\n","    print(\"label          : \", train_dataset[n][2],\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rv18VfTFgUH6","executionInfo":{"status":"ok","timestamp":1657860507598,"user_tz":-540,"elapsed":10,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"e7983b4b-82ea-4d76-986f-37a0ed6bd140"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["train_dataset[ 0 ]\n","token_ids      :  [0, 3, 7254, 3841, 2062, 636, 3711, 12717, 2178, 2062, 11980, 2062, 2, 3, 7254, 3841, 2200, 11980, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  0 \n","\n","train_dataset[ 1 ]\n","token_ids      :  [0, 3911, 2377, 2366, 1521, 3061, 4785, 1282, 2955, 3308, 3515, 2170, 22, 2532, 5675, 2, 3911, 2377, 2366, 1525, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  2 \n","\n","train_dataset[ 2 ]\n","token_ids      :  [0, 3911, 2377, 2366, 1521, 3061, 4785, 1282, 2955, 3308, 3515, 2170, 22, 2532, 5675, 2, 1282, 2955, 3308, 2052, 3944, 11580, 2359, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  1 \n","\n","train_dataset[ 3 ]\n","token_ids      :  [0, 3911, 2377, 2366, 1521, 3061, 4785, 1282, 2955, 3308, 3515, 2170, 22, 2532, 5675, 2, 3911, 2377, 2366, 5105, 2318, 831, 717, 2886, 2069, 575, 555, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  1 \n","\n","train_dataset[ 4 ]\n","token_ids      :  [0, 10522, 2548, 2500, 6328, 2170, 6189, 5916, 4015, 2116, 1039, 2219, 3606, 18, 2, 10522, 2548, 2500, 6328, 27135, 5916, 4015, 1642, 2015, 2259, 4258, 2219, 3606, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  2 \n","\n"]}]},{"cell_type":"code","source":["# 검증 데이터셋\n","val_dataset = NLIDataset(datasets[\"validation\"])\n","\n","for n in range(5):\n","    print(\"val_dataset[\",n,\"]\")\n","    print(\"token_ids      : \", val_dataset[n][0])\n","    print(\"attention_mask : \", val_dataset[n][1])\n","    print(\"label          : \", val_dataset[n][2],\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0VH-ac9iu1Y","executionInfo":{"status":"ok","timestamp":1657860507598,"user_tz":-540,"elapsed":7,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"05761cba-ed79-4040-ab04-3c07040d43e8"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["val_dataset[ 0 ]\n","token_ids      :  [0, 25313, 2377, 2031, 2073, 20812, 2116, 1513, 2259, 1129, 24094, 20812, 27135, 9753, 2052, 3662, 11800, 18, 2, 3711, 1129, 27135, 2119, 9753, 2073, 5040, 3598, 3606, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  2 \n","\n","val_dataset[ 1 ]\n","token_ids      :  [0, 3633, 2211, 2052, 3655, 3704, 31302, 5153, 2530, 4087, 4671, 2371, 2062, 18, 2, 3633, 2211, 2052, 3655, 3704, 31302, 5153, 2530, 2052, 1039, 2886, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  2 \n","\n","val_dataset[ 2 ]\n","token_ids      :  [0, 3633, 2211, 2052, 3655, 3704, 31302, 5153, 2530, 4087, 4671, 2371, 2062, 18, 2, 6025, 3633, 2211, 2052, 3655, 3704, 31302, 5153, 2530, 4087, 1415, 2359, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  1 \n","\n","val_dataset[ 3 ]\n","token_ids      :  [0, 3633, 2211, 2052, 3655, 3704, 31302, 5153, 2530, 4087, 4671, 2371, 2062, 18, 2, 3633, 2211, 2052, 3655, 3704, 31302, 2170, 4671, 2255, 13929, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  0 \n","\n","val_dataset[ 4 ]\n","token_ids      :  [0, 3633, 2624, 2170, 4515, 11336, 2031, 2154, 3774, 2205, 2259, 12174, 2145, 1347, 2472, 2343, 7285, 1513, 5515, 18, 2, 4515, 11336, 2031, 2073, 12174, 2145, 1347, 2472, 2343, 2138, 3774, 2085, 1295, 1513, 2219, 3606, 18, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","label          :  0 \n","\n"]}]},{"cell_type":"markdown","source":["**데이터로더 구축**"],"metadata":{"id":"SVRsJatwgA4A"}},{"cell_type":"code","source":["# collate_fn 구성\n","def collate_batch(batch):\n","    token_ids = [item[:][0] for item in batch]\n","    attention_mask = [item[:][1] for item in batch]\n","    label_ids = [item[:][2] for item in batch]\n","\n","    return torch.LongTensor(token_ids), torch.LongTensor(attention_mask), torch.LongTensor(label_ids)"],"metadata":{"id":"qNJexSZPo06m","executionInfo":{"status":"ok","timestamp":1657860507599,"user_tz":-540,"elapsed":6,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn = collate_batch, batch_size=8)\n","val_dataloader = DataLoader(val_dataset, collate_fn = collate_batch, batch_size=16)"],"metadata":{"id":"klpNKj8Gf-zN","executionInfo":{"status":"ok","timestamp":1657860507599,"user_tz":-540,"elapsed":6,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 데이터로더 확인\n","sample_data = iter(train_dataloader)\n","sample_ids = next(sample_data)\n","\n","token_ids, attention_mask, label_ids = sample_ids\n","\n","print(\"first item of batch (train_dataloader)\")\n","print(\"token_ids \\n\", token_ids[:][0],\"batch size : \", token_ids.size(),\"\\n\")\n","print(\"attention_mask \\n\", attention_mask[:][0], \"batch size : \", attention_mask.size(),\"\\n\")\n","print(\"label_ids \\n\", label_ids[:][0], \"batch size : \", label_ids.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJkfULkQjJg5","executionInfo":{"status":"ok","timestamp":1657860508445,"user_tz":-540,"elapsed":851,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"83c0525f-d1e5-4192-8be4-7c3f6dbd526f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["first item of batch (train_dataloader)\n","token_ids \n"," tensor([    0,  5443, 23990,  2122,  1370,  5674,  1932,  2020,  2122,  1370,\n","         2116,  5795,  5488,  3261,  2069,  4049,  2371,  2062,    18,     2,\n","        23990,  2122,  2073,  1932,  2020,  2122,  2079,  5674, 28674,    18,\n","            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1]) batch size :  torch.Size([8, 64]) \n","\n","attention_mask \n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) batch size :  torch.Size([8, 64]) \n","\n","label_ids \n"," tensor(2) batch size :  torch.Size([8])\n"]}]},{"cell_type":"markdown","source":["# 모델 학습\n","\n","**모델 정의**"],"metadata":{"id":"CDJEAnCFi1F8"}},{"cell_type":"code","source":["# RoBERTa를 포함한 신경망 모형\n","class NLIModel(torch.nn.Module):\n","    def __init__(self, pretrained_model, token_size, num_labels): \n","        super(NLIModel, self).__init__()\n","        \n","        self.token_size = token_size\n","        self.num_labels = num_labels\n","        self.pretrained_model = pretrained_model\n","\n","        # 분류기 정의\n","        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # BERT 모형에 입력을 넣고 출력을 받음\n","        outputs = self.pretrained_model(input_ids, attention_mask)\n","        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져옴\n","        bert_clf_token = outputs.last_hidden_state[:,0,:]\n","        # 3개의 라벨로 분류\n","        outputs = self.classifier(bert_clf_token)\n","\n","        return outputs\n","\n","# token_size는 BERT 토큰과 동일\n","model = NLIModel(roberta_model, token_size=roberta_model.config.hidden_size, num_labels=3)"],"metadata":{"id":"6CCJbw9ai04s","executionInfo":{"status":"ok","timestamp":1657863467117,"user_tz":-540,"elapsed":2,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["**파라미터 설정**"],"metadata":{"id":"Uf142OdRnHsF"}},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","import torch.nn.functional as F\n","import time\n","\n","# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# 옵티마이저 AdamW로 설정\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # 가중치 감쇠 설정\n","criterion = torch.nn.CrossEntropyLoss()    # 멀티클래스이므로 크로스 엔트로피를 손실함수로 사용 -> RoBERTa 코드 내 포함되어있음\n","\n","num_epochs = 3      # 학습 epoch를 3회로 설정\n","\n","total_training_steps = num_epochs * len(train_dataloader)\n","# 학습 스케줄러 설정\n","scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n","                                            num_training_steps=total_training_steps,\n","                                            num_warmup_steps=200)\n","\n","step = 0\n","eval_steps = 500"],"metadata":{"id":"lMCVrMb3mYr0","executionInfo":{"status":"ok","timestamp":1657863468888,"user_tz":-540,"elapsed":319,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["model.to(device)  \n","model.train()     # 학습모드"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jqsmfiHkmtc0","executionInfo":{"status":"ok","timestamp":1657863470403,"user_tz":-540,"elapsed":7,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"4110a83c-fab7-47b2-9caa-0d234d84e025"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NLIModel(\n","  (pretrained_model): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["**학습 진행**"],"metadata":{"id":"vpkSkbJ67hqD"}},{"cell_type":"code","source":["# GPU 캐시 비우기 (GPU 메모리 확보)\n","torch.cuda.empty_cache()"],"metadata":{"id":"l44mciLYAlbc","executionInfo":{"status":"ok","timestamp":1657862922093,"user_tz":-540,"elapsed":296,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0\n","    avg_loss = 0.0\n","    \n","    for batch_idx, samples in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()       # optimizer 초기화(Gradient)\n","\n","        # 모델 입력 텐서 GPU에 올리기\n","        token_ids, attention_mask, label_ids = samples\n","\n","        token_ids = token_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        label_ids = label_ids.to(device)\n","\n","        out = model(\n","            input_ids=token_ids,\n","            attention_mask=attention_mask,\n","            )\n","\n","        loss = criterion(out, label_ids)        # out: [8, 3], label: [8] -> 어떤 방식으로 처리할까?\n","        loss.backward()\n","        optimizer.step()\n","\n","        avg_loss += loss.item()\n","        \n","        step += 1\n","        if step % eval_steps == 0:  # eval_steps 마다 경과한 시간과 loss를 출력\n","            with torch.no_grad():   # 학습 X (그래디언트 계산 X)\n","                val_loss = 0\n","                model.eval()        # 평가모드로 전환\n","\n","                for batch_idx, samples in enumerate(tqdm(val_dataloader)):\n","\n","                    token_ids, attention_mask, token_type_ids, label_ids = samples\n","\n","                    token_ids = token_ids.to(device)\n","                    attention_mask = attention_mask.to(device)\n","                    label_ids = label_ids.to(device)\n","                    \n","                    out = model(\n","                        input_ids=token_ids,\n","                        attention_mask=attention_mask,\n","                        )\n","\n","                    loss = criterion(out.reshape(8), label_ids)  \n","                    val_loss += loss\n","\n","                avg_val_loss = val_loss / len(val_dataloader)\n","\n","            avg_train_loss = train_loss / eval_steps    # eval_steps의 평균 loss 계산\n","            \n","            print('Step %d, train loss: %.4f, validation loss: %.4f' \n","                  % (step, avg_train_loss, avg_val_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"Ictc3lhk7g-G","executionInfo":{"status":"error","timestamp":1657863613713,"user_tz":-540,"elapsed":317,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"4b704a05-7dd6-4c56-cccb-7a91724244b9"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/3125 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-4d53f19f763d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# out: [8, 3], label: [8] -> 어떤 방식으로 처리?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'argmax'"]}]}]}