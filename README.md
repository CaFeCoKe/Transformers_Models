# Transformers_Models
 
Transformers의 다양한 언어 모델(BERT, GPT 제외)을 공부하기 위한 repository이며, 최대한 논문을 통해 공부하고자 합니다.

- LLM (Large Language Models)의 발전 역사 <br>
![qr_version](https://github.com/CaFeCoKe/Transformers_Models/assets/86700191/e4b218ad-0202-4a51-8e12-9b409fd78046)


- Autoencoding (Encoder) 기반
  - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
  - [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf)
  - [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB)
  - [XLM: Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291.pdf)


- AutoRegressive (Decoder) 기반
  - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)
  - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)


- Sequence-to-Sequence (Encoder-Decoder) 기반
  - [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)
  - [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)


- 참고할만한 사이트
  - [Mooler0410's  LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide)